<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>On the Value of Cross-Modal Misalignment in Multimodal Representation Learning</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">On the Value of Cross-Modal Misalignment in Multimodal Representation Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yichaocai.com/" target="_blank">Yichao Cai*, </a></span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/yuhangliu/homepage" target="_blank">Yuhang Liu*, </a></span>
                  <span class="author-block">
                    <a href="https://erdungao.github.io/" target="_blank">Erdun Gao, </a></span>
                  <span class="author-block">
                    Tianjiao Jiang </span>
                  <span class="author-block">
                    <a href="https://zzhang.org/" target="_blank">Zhen Zhang, </a></span>
                  <span class="author-block">
                    <a href="https://researchers.adelaide.edu.au/profile/anton.vandenhengel" target="_blank">Anton Van Den Hengel, </a></span>
                    <span class="author-block">
                      <a href="https://cs.adelaide.edu.au/~javen/" target="_blank">Javen Qinfeng Shi</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">* Equal Contribution<br>Australian Institute for Machine Learning (AIML), The University of Adelaide</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.10143" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YichaoCai1/crossmodal_mislaignment" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2504.10143" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <p class="content has-text-justified">
      This work introduces a unified framework for understanding cross-modal misalignment in vision-language learning. Despite prevailing assumptions, real-world image-text pairs often contain only partially overlapping semantics. Using a latent variable model (LVM), we formalize two common misalignment mechanisms—selection and perturbation biases—and analyze their impact on contrastive learning methods like CLIP. Our theory reveals that multimodal contrastive learning inherently extracts the unbiased shared semantics. Empirical studies across synthetic and real datasets confirm these insights.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">1. Motivation</h2>
    <p class="content">
      Multimodal contrastive learning aims to align image and text embeddings in a shared space. This assumes that the pairs are semantically matched. However, in practice, the matching is often noisy: some semantics are missing (selection bias), and some are replaced or incorrect (perturbation bias). Surprisingly, such misalignment doesn't always harm performance—sometimes, it improves robustness. This paper seeks to understand and exploit that phenomenon.
    </p>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">2. Theoretical Framework</h2>
    <p class="content">
      The authors propose a flexible latent variable model, where the full semantics of each sample are encoded in latent variables. The model generates images and texts via different deterministic paths, with the possibility of omitting or altering variables. Two types of biases are defined:
    </p>
    <ul>
      <li><strong>Selection Bias (θ):</strong> Certain (potental) shared semantics are excluded from the text modality.</li>
      <li><strong>Perturbation Bias (ρ):</strong> Spurious semantics (not identical to the original image sample) are added to text across positive pairs.</li>
    </ul>
    <br><br>
    <figure class="image">
      <img src="static/images/lvm.png" alt="LVM Diagram">
    </figure>
    <p class="subtitle has-text-centered">Figure:  Illustration of the proposed latent variable model (left), with misalignment across modalities modeled via selection and perturbation bias.</p>
    <br><br>
    <p class="content">
      <strong> Main Results </strong>:The main theoretical result demonstrates that contrastive learning consistently identifies only the shared, unbiased components of the underlying semantic variables—regardless of the causal relationships among them. This explains why CLIP-like models maintain strong generalization performance even when trained on semantically noisy or misaligned image-text pairs. These findings also highlight the critical importance of careful dataset design in multimodal contrastive learning.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">3. Experiments</h2>
    <p class="content">
      The paper evaluates the theory using numerical simulations, real-world (MPI-) and controlled synthetic datasets (Causal3DIdent). Metrics include zero-shot classification accuracy and semantic variable prediction accuracy (R², MCC).
    </p>

    <figure class="image">
      <img src="static/images/causal3dIdent.png" alt="Causal3DIdent samples">
    </figure>
    <p class="subtitle has-text-centered">Figure: Latent causal model underlying the generation of image samples in the Causal3DIdent dataset.</p>
    <figure class="image">
      <img src="static/images/misalignment_settings.png" alt="Performance curves">
    </figure>
    <p class="subtitle has-text-centered">Table: Misalignment settings for text generation of Causal3DIdent dataset.</p>
    <figure class="image">
      <img src="static/images/performance.png" alt="Performance curves">
    </figure>
    <p class="subtitle has-text-centered">Figure: Predicting semantic variables under misalignment using image features. </p>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">4. Implications and Discussion</h2>
    <p class="content">
    This work challenges the assumption that perfect semantic alignment is necessary for effective multimodal learning. In fact, some degree of misalignment—such as selection or perturbation biases—can encourage models to focus on invariant, shared semantics. Our theoretical results show that contrastive learning recovers these unbiased components even without strong assumptions about the causal structure of the data, helping explain the robustness of CLIP-like models.<br><br>
    Beyond performance, our findings have broader implications. Supervision—particularly text—acts as an epistemic filter, shaping which aspects of the visual world are represented and learned. Biases in annotation reflect implicit value judgments about what is relevant or important. Rather than treating these biases as mere noise, they can be studied as signals—providing behavioral priors that inform what humans prioritize. This reframes dataset design as both a technical and ethical practice: one that impacts not only generalization, but also what concepts are visible to the model in the first place.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@article{cai2025misalignment,
  title={On the Value of Cross-Modal Misalignment in Multimodal Representation Learning},
  author={Cai, Yichao and Liu, Yuhang and Gao, Erdun and Jiang, Tianjiao and Zhang, Zhen and van den Hengel, Anton and Shi, Javen Qinfeng},
  journal={arXiv preprint arXiv:2504.10143},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Based on the <a href="https://nerfies.github.io">Nerfies</a> project page template. Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
    </p>
  </div>
</footer>

</body>
</html>